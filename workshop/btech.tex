\vspace{-.35cm}

\babak{Introduce ZaliQL here}
\section{Basic Techniques}
\label{sec:BasicTechniques}


% \subsection{Declarative Specification of the Matching Methods}
% \label{sec:BasicTechniquesdef}



In this section we review the matching and subclassification
techniques used in causal inference and propose several relational
encodings, discussing their pros and cons.  Historically, matching was
introduced before subclassification, so we present them in this order.
Subclassification is the dominant technique in use today: we will
discuss optimizations for subclassification in the next section.

We consider a single relation $\rel(ID,T,\cv,Y)$,
where $ID$ is an integer-valued primary-key, $T$ and $\cv$
respectively denote the treatment and covariate attributes as
described in the NRCM (cf. Section \ref{subsec:causalitystatistics}),
and $Y$ represent the available outcome, i.e., $Y=Y(z)$ for iff $T=z$. For each matching method, we define a view over $\rel$ such that
materializing the extension of the view over any instance of $\rel$
computes a corresponding matched subset of the instance.


\vspace{0.3cm}


\ignore{

\begin{figure*}
\centering

\begin{subfigure}[t]{.6\textwidth}
\centering


  \begin{alltt} \scriptsize
CREATE VIEW \(\nnm\sp{r}\) AS
WITH tmp0 AS
  (SELECT treated.ID t,
          control.ID c,
          |treated.ps - control.ps|  distance
   FROM \(\rel\) control,
        \(\rel\) treated
   WHERE control.t=0
     AND treated.t=1
     AND |treated.ps - control.ps|< \(caliper\)),
      tmp2 AS
  (SELECT *, rank() OVER (PARTITION BY control.ID
                       ORDER BY distance) rank
   FROM tmp0)
SELECT *
FROM tmp2
WHERE rank \(\leq k\);
\end{alltt}

\begin{subfigure}[t]{.6\textwidth}
  \begin{alltt} \scriptsize
SELECT count(*)
FROM test c,
     test t
WHERE c.t=0
  AND t.t=1
  AND c.ID!=t.ID
  AND c.ps <-> t.ps<0.1
  AND
    (SELECT count(*)
     FROM test z
     WHERE z.t=0
       AND t.ps <-> z.ps < c.ps <-> t.ps)<3 ;
\end{alltt}
\end{subfigure}
\label{fig:causal:inference}
\caption{$K$-NNM with replacement}\label{fig:fig_a}
\end{subfigure}
%
\begin{subfigure}[t]{.4\textwidth}
\centering
  \begin{alltt} \scriptsize
CREATE VIEW \(\nnm\) AS
(WITH tmp0 AS (
SELECT *,
       max(ID) OVER w grp,
       random() weight,
FROM \(\rel\) WINDOW w AS (PARTITION BY \(x\sb{1},\ldots,x\sb{k}\)),
     tmp1 AS
  (SELECT *,
           rank() OVER (PARTITION BY grp, ps,
           treatment ORDER BY weight)
   FROM tmp0),
     tmp2 AS
  (SELECT *,
           max(t) OVER w maxt,
           min(t) OVER w mint
   FROM tmp1 WINDOW w AS (PARTITION BY grp,
                                       rank,
                                       ps))
SELECT ID,t,\( \cv\),o
FROM tmp2
WHERE maxt1!=mint1)
\end{alltt}

\caption{$K$-NNM without replacement}\label{fig:fig_b}
\end{subfigure}

\medskip


\begin{minipage}[t]{.4\textwidth}
\caption{SQl implementation of variant $K$-NNM.}
\end{minipage}

\end{figure*}
}

\begin{figure}
  \centering
\begin{alltt} \scriptsize
CREATE VIEW \(\nnmwr\) AS
SELECT *
   FROM \(\rel\) AS control,\(\rel\) AS treated
WHERE control.T=0  AND treated.T=1
  AND \(\delta\)(treated\(.\cv\),control\(.\cv)\) < \(caliper\)
  AND (SELECT count(*)
     FROM \(\rel\) AS z
     WHERE z.T=0
       AND \(\delta\)(treated\(.\cv\),z\(.\cv)\) < \(\delta\)(treated\(.\cv\),control\(.\cv)\) \(\leq k\))
\end{alltt} \vspace{-.5cm}
\center{\bf (a)  Anti-join based}
\vspace{-.4cm}
\begin{alltt} \scriptsize
CREATE VIEW \(\nnmwr\) AS
WITH potential_matches AS
  (SELECT treated.ID AS tID,control.ID AS cID,
         \(\delta\)(treated\(.\cv\),control\(.\cv)\)  AS distance
   FROM \(\rel\) AS control, \(\rel\) AS treated
   WHERE control.T=0 AND treated.T=1
     AND \(\delta\)(treated\(.\cv\),control\(.\cv)\) < \(caliper\)),
      ranked_potential_matches AS
  (SELECT *, ROW_NUMBER() OVER (PARTITION BY tID
                       ORDER BY distance) AS order
   FROM potential_matches)
SELECT *
FROM ranked_potential_matches
WHERE order \(\leq k\);
\end{alltt}
\vspace{-.5cm}
\center{\bf (b) Window function based}
  \caption{\bf SQL implementation of NNMWR.}\label{fig:nnmwr}
\end{figure}
\vspace{-.35cm}
\newpage

\babak{ We should decide whether we want to include the NNM stuff}
\subsection{Nearest Neighbor Matching}
\label{sec:nnm}
The most common matching method is that of $k:1$ nearest neighbor
matching (NNM) \cite{Rubin1983b,ho2005,Stuart10}. This method selects
the $k$ nearest \ignore{(wrt. one of the distance metrics discussed in
  Section \ref{sec:matching})} control matches for each treated
unit and can be done with or without replacement; we denote
them respectively by NNMWR and NNMNR.  In the former case, a control
unit can be used more than once as a match, while in the latter case
it is considered only once. Matching with replacement can often
decrease bias because controls that look similar to the treated units
can be used multiple times.  \ignore{This method is helpful in settings where
there are few control units available.} However, since control units
are no longer independent, complex inference is required to estimate
the causal effect \cite{dehejia99}. In practice, matching is usually
performed without replacement.  Notice that NNM faces the risk of bad
matches if the closest neighbor is far away. This issue can be
resolved by imposing a tolerance level on the maximum distance, known
as the {\em caliper} (see e.g., \cite{lunt2014selecting}). \ignore{There are some rules of thumb for choosing the
calipers (see e.g., \cite{lunt2014selecting}).}


{\bf NNM With Replacement}
We propose two alternative ways for computing  NNMWR in SQL, shown in
Figure \ref{fig:nnmwr}. In Figure \ref{fig:nnmwr}(a), each treated unit is joined with $k$ closest control units that are closer than the caliper. In this solution, nearest control units are identified by means of an anti-join. \ignore{Note that when $k=1$ the aggregate expression may be replaced by NOT EXISTS.}  In Figure \ref{fig:nnmwr}(b), all potential matches and their distances are identified by
joining the treated with the control units that are closer than
the caliper. Then, this set is sorted into ascending order of
distances.  In addition, the order of each row in the sorted set is identified
using the window function {\verb|ROW_NUMBER|}. Finally, all units with the order of less than or equal to $k$ are selected as the matched units.


The ani-join based statement requires a three-way join. \ignore{However, in a particular case when
  $k=1$ and the caliper is zero, this solution can becomes linear for
  matching based on propensity score.} The window function based
solution has a quadratic complexity. It requires a nested-loop to
perform a spatial-join and a window aggregate to impose
minimality. Note that window functions are typically implemented in
DBMS using a sort algorithm, and even more efficient algorithms have
been recently proposed~\cite{Neumann15}.






\begin{figure}
  \centering
\begin{alltt} \scriptsize
CREATE VIEW \(\nnmnr\)
AS WITH potential_matches AS
  (SELECT treated.ID AS tID, control.ID AS cID,
          \(\delta(treated.\cv,control.\cv)\)  AS distance
   FROM \(\rel\) AS control, \(\rel\) AS treated
   WHERE control.T=0 AND treated.T=1
     AND \(\delta(treated.\cv,control.\cv)\) < \(caliper\))),
            ordered_potential_matches AS
  (SELECT *, ROW_NUMBER() over (ORDER BY distance) AS order
   FROM potential_matches)
SELECT *
FROM ordered_potential_matches AS rp
WHERE NOT EXISTS
    (SELECT *
     FROM ordered_potential_matches AS z
     WHERE z.order < rp.order AND z.cID=rp.cID)
  AND (SELECT count(*)
     FROM ordered_potential_matches AS rp
     WHERE z.order < rp.order AND z.tID=rp.tID)\( \leq k\);
\end{alltt} \vspace{-.5cm}
  \caption{\bf SQL implementation of NNMNR}\label{fig:nnmnr}
\end{figure}


{\bf NNM Without Replacement} \ignore{Expressing NNMNR in a declarative manner
can be complicated. In fact,} This method aims
to minimize the average absolute distance between matched units and can performed in either greedy or optimal manner. The latter is called {\em optimal matching}
\cite{Rosenbaum93}. Before we describe our proposed SQL implementation for NNMWR, we prove
that optimal matching is not expressible in SQL: this justifies
focusing on approximate matches.  For our inexpressibility result,
notice that in the special case when $k=1$ NNMWR is the {\em weighted
  bipartite graph matching problem (WBGM)}, which is defined as
follows: given a bipartite graph $G=(V,E)$ and a weight function
$w: E \rightarrow \mathbb{R}_{>0}$, find a set of vertex-disjoint
edges $M \subseteq E$ such that $M$ minimise the total weight
$w(M) = \sum_{e \in M} w(e)$.  The exact complexity of this problem
is unknown (see, e.g. \cite{Avis83}), however we prove a \NLOGSPACE\
lower bound:

\vspace{-.2cm}
\begin{proposition} \label{pro:om}
Computing  maximum weight matching for  weighted bipartite graphs is hard for \NLOGSPACE.
\end{proposition}

\begin{proof} The following {\em Graph Reachability Problem} is known
  to be \NLOGSPACE\ complete: given a directed graph $G(V,E)$ and two
  nodes $s,t$, check if there exists a path from $s$ to $t$.  We prove
  a reduction from graph reachability to the {\em bipartite perfect
    matching problem} which is a special case of optimal WBGM. For
  that we construct the graph $G'$ with $V= V \cup V'$ where, $V'$ is
  a copy of $V$ with primed labels and
  $E'= \{(x,x')| \forall x \in V -\{s,t\} \} \cup \{(x,y')| \forall
  (x,y) \in E\} \cup \{(t,s')\}$.
  Notice that the subset $\setof{(x,x')}{x \in V} \subseteq E$ is
  almost a perfect matching, except that it misses the nodes $s, t'$.
  We prove: there exists a path from $s$ to $t$ in $G$ iff $G'$ has a
  perfect matching.  First assume $P=s, x_1, x_2, \ldots, x_m, t$ is a
  path in $G$. Then the following forms a perfect matching in $G'$:
  $M=\set{(s,x_1'), (x_1,x_2'), \ldots, (x_{m},t'), (t,s')} \cup
  \setof{(y,y')}{y \not\in \set{s,x_1, \ldots, x_m,t}}$.
  Conversely, assume $G'$ has a perfect matching. Write
  $f : V \rightarrow V'$ the corresponding bijection, i.e. every $x$
  is matched to $y'=f(x)$.  Denoting the nodes in $V$ as
  $V=\set{x_1, \ldots, x_n}$, we construct inductively the following
  sequence: $x_{i_1}' = f(s)$, $x_{i_2}' = f(x_{i_3})$, \ldots,
  $x_{i_{k+1}}' = f(x_{i_k})$.  Then $i_1, i_2, \ldots$ are distinct
  (since $f$ is a matching), hence this sequence must eventually reach
  $t'$: $t' = f(x_{i_m})$.  Then
  $s,x_{i_1},x_{i_2}, \ldots, x_{i_m}, t$ forms a path from $s$ to $t$
  in $G$.  This completes the proof.
\end{proof}

The proposition implies that optimal matching is not expressible in
SQL without the use of recursion.  Optimal matching can be solved in
\PTIME using, for example, the {\em Hungarian} algorithm, which, in
theory, could be expressed using recursion in SQL.  However, optimal
matching is rarely used in practice and, in fact, it is known that it
does not in general perform any better than the greedy NNM (discussed
next) in terms of reducing degree of covariate imbalance
\cite{Rosenbaum93}.   For that reason, we did not implement optimal
matching in our system.


1:1 NNMWR can be approximated with a simple greedy algorithm that
sorts all edges of the underlying graph in ascending order of weights
and iterates through this sorted list, marking edges as ``matched"
while maintaining the one-to-one invariant. \ignore{This algorithm can return
a maximal matching that is at least $\frac{1}{2}$-optimal
\cite{Avis83}.} Figure \ref{fig:nnmnr} adopts this greedy algorithm to
express $1:k$ NNMWR in SQL.  This algorithm is very similar to that of
NNMWR in Figure \ref{fig:nnmwr}(b), with the main difference that in
the matching step it imposes the restriction that a control unit is
matched with a treated unit only if it is not not already matched with
another treated with a lower order.  This solution also has a
quadratic complexity.

{\bf Choosing the distance function} We briefly discuss now the choice
of the distance function $\delta$ in NNM (see Fig.~\ref{fig:metrics}).
The propensity score distance is by far the most prominent metric in
NNM.  However, it has been the subject of some recent criticisms
\cite{king15}.  It has been shown that, unlike other matching methods,
in propensity score matching the imbalance reduction is only
guaranteed across the spectrum of all samples. In observational
settings, we typically have only one sample, so other matching methods
dominate propensity score matching \cite{king15}. An alternative is to use the mahalanobis distance.  This has been
shown to exhibit some odd behavior when covariates are not normally
distributed, when there are relatively large number of covariates, or
there are dichotomous covariates
\cite{rosenbaum2002observational}. Therefore, this method has a
limited practical applicability.


We should mention that there is huge literature in the database
community on finding the nearest neighbor. In fact this type of
queries are subject of an active research and development efforts in
the context of spatial-databases (see, e.g.,
\cite{obe2015postgis}). Our work is different from these efforts in that: 1) much of the work in this area has focused on finding sub-linear algorithm for identifying nearest neighbors of a single data item (e.g., by using spatial-index). In contrast, in our setting
we need to find all nearest neighbors, which is by necessity quadratic; 2) these works resulted in specialized algorithm,
implemented in general purposed languages. In contrast, we focus on finding a representation in SQL, in order to integrate causal
 analysis with other data analytic operations.  \ignore{ Recent developments in this context are
applicable to our problem.  However, due to the enumerated
shortcomings of NNM based on Mahalanobis and propensity score
distance, this paper focuses on other matching methods.  \dan{which
  other matching methods?  also, if we don't use the current matching
  methods then why do we mention them?  and if research on NN is
  relevant, then what is novel here?}}









\begin{figure}
\begin{alltt} \scriptsize
CREATE VIEW \(\sbc\) AS
(WITH tmp0 AS
  (SELECT *. ntile(\(n\)) over w subclass,
   FROM \(\rel\) window w AS (ORDER BY ps))
SELECT ID, T, \(\ccv\), Y, subclass,
             max(T) over w maxT, min(T) over w minT
FROM tmp0  window w AS (PARTITION BY BY subclass)
WHERE maxT!=minT)
\end{alltt}
\vspace{-0.3cm}
  \caption{\bf{SQL implementation of subclassification based on the
      propensity score.}}\label{fig:subpr}
\end{figure}

\vspace{-.2cm}

\subsection{Subclassification}
\label{sec:sub}
It is easy to see that NNM does not necessarily use all the data,
meaning that many control units despite being in the range of a
treatment unit are discarded.  In subclassification, the aim is to
form subclasses for which, the distribution of covariates for the
treated and control groups are as similar as possible. \ignore{The use of
subclassification for matching can be traced back to
\cite{cochran1968effectiveness}, which examined this method on a
single covariate (age), investigating the relationship between lung
cancer and smoking.} It is shown that using just five subclasses based
on univariate continues covariates or propensity score removes over
$90\%$ of covariates imbalance \cite{cochran1968effectiveness,
  rosenbaum1984reducing}.

{\bf Subclassification based on the propensity score} We describe the
SQL implementation of subclassification based on $n$ quintiles of the
propensity score in Figure \ref{fig:subpr}.  We assumed that $\rel$
includes another attribute $ps$ for the propensity score of each unit;
the value of $ps$ needs to be learned from the data, using logistic
regression \cite{Rubin1983b}.  The SQL query seeks to partition
the units into five subclasses with propensity scores as equal as
possible using the window function {\verb|ntile|} and ensure the overlap within each subclass. \ignore{. Finally,
subclasses for which the maximum and minimum of the treatment are not
equal retained (the discarded subclasses do not enjoy the overlap
assumption).} This solution has the order of $nlog(n)$ if the window
function computed using a sort algorithm.


%%%%  we haven't defined univariates; i don't know what they are
% Subclassification based on univariate continues covariates is a
% particular case of coarsen exact matching, which is discussed in the
% next section.





\begin{figure}
\begin{alltt} \scriptsize
CREATE VIEW \(\crele\) AS
(SELECT *, (CASE WHEN \(x\sb{1}<c\sb{1}\)  THEN \(1 \ldots\)
                 WHEN \(x\sb{1}>c\sb{(k\sb{1}-1)}\) THEN \(k\sb{1}\)) AS \(cx\sb{1}\),
              . . .
           (CASE WHEN \(x\sb{n}<c\sb{n}\)  THEN \(1 \ldots\)
                 WHEN \(x\sb{n}>c\sb{(k\sb{n}-1)}\) THEN \(k\sb{n}\)) AS \(cx\sb{n}\)
from \(\rel\));
\end{alltt} \vspace{-.2cm} \hspace{0.7cm}
{(a)Coarsening wrt. a set of prespecified cutpoints}
\vspace{-.3cm}
\begin{alltt} \scriptsize
CREATE VIEW \(\cem\) AS
SELECT ID, T, \(\ccv\), Y, subclass
FROM
  (SELECT *,
          max(ID) OVER w AS subclass, max(T) OVER w AS minT,
          min(T) OVER w AS maxT
   FROM \(\crele\)
   WINDOW w (PARTITION BY \(\ccv\)))
WHERE mint!=maxt
\end{alltt}\vspace{-.2cm} \hspace{2.5cm}
(b)Window function based
\vspace{-.3cm}
\begin{alltt} \scriptsize
CREATE VIEW \(\cem\) AS
WITH subclasses AS
  (SELECT *,
          max(ID) OVER w subclass, max(T) OVER w AS minT,
          min(T) OVER w AS maxT
   FROM \(\crele\)
   Group by \(\ccv\))
SELECT ID, T, \(\ccv\), Y, subclass
FROM subclasses,\(\crele\)
WHERE subclasses.\(\ccv\)=\(\crele\).\(\ccv\)  AND  minT!=maxT

\end{alltt}\vspace{-.2cm}\hspace{2.5cm}
(c) Group-by based
\vspace{-.1cm}
  \caption{\bf{SQL implementation of CEM.}}\label{fig:cem}
\end{figure}


% \subsection{Coarsen Exact Matching (CEM)}
% \label{sec:cem}

{\bf Coarsening Exact Matching (CEM)} This method as proposed
recently in \cite{IacKinPor09},  is a particular form of subclassification in which the vector of covariates $\cv$ is
coarsened according to a set of user-defined cutpoints or any
automatic discretization algorithm.  Then all units with similar
coarsened covariates values are placed in unique subclasses. All
subclasses with at least one treated and one control unit are retained
and the rest of units are discarded.  Intuitively, this is a group-by
operation, followed by eliminating all groups that have no treated, or
no control unit. \ignore{Subclassification based
  on univariate continues covariate (cf Section \ref{sec:sub}) can bee
  seen as a particular form of CEM.}

For each attribute $x_i \in \cv$, we assume a set of cutpoints
$c_i=\{c_{1} \ldots c_{(k_i-1)}\}$ is given, which can be used to
coarsen $x_i$ into $k_i$ buckets. The view $\crele$, shown in Figure
\ref{fig:cem}(a), defines extra attributes
$\ccv=\{\ccvi_1 \ldots \ccvi_n\}$, where $\ccvi_i$ is the coarsened
version of $x_i$. Two alternative SQL implementations of CEM are
represented in Figure \ref{fig:cem}(b) and (c).  The central idea in
both implementations is to partition the units based on the coarsened
covariates and discard those partitions that do not enjoy the overlap assumption. 
\corey{assumption or condition here?}
\ignore{Then, each partition in which the maximum and minimum of
the unit treatments are equal are discarded. This is because they do
not enjoy the overlap assumption.} Note that the maximum of unit
$ID$s in each partition is computed and used as its unique
identifier. The window function based solution has the order of
$nlog(n)$, if the window aggregate computed using a sort
algorithm. The group-by based solution can becomes linear if the join
is performed by a hash-join.


Several benefits of CEM has been proposed in \cite{IacKinPor09}. For
instance, unlike other approaches, the degree of imbalance is bounded
by the user (through choosing proper cut-points for covariates
coarsening), therefore the laborious process of matching and checking
for balance is no longer needed.  \ignore{More importantly, this approach
meets the {\em congruous principle}, which assert that there should be
a congruity between analysis space and data space. Notice that
Mahalanobis distance and propensity score, project a vector from
multidimensional space into a scalar value. It has been argued that
methods violating the congruous principle may lead to less robust
inference with sub-optimal and highly counterintuitive properties
\cite{IacKinPor09}.} The remainder of this paper focuses on
optimization techniques to speed up CEM.


\vspace{-2mm} 